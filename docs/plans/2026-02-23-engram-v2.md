# Implementation Plan: Engram V2 — Human-Like Memory for Claude Code

**Date:** 2026-02-23
**Status:** Reviewed & Updated (post code-review)
**Research:** [architecture](../research/2026-02-23-human-like-memory-architecture.md) | [validation](../research/2026-02-23-engram-validation.md) | [constraints](../research/2026-02-23-continuous-memory-constraints.md)

---

## Feature Summary

**Idea:** Automatic memory capture from all sessions + continuous "memory bite" recollections that Claude can expand or dismiss — like real human memory.

**Problem:** Current memory depends on the agent choosing to save. Entire projects (VibeTunnel) get lost because the agent never saved anything.

**User:** Ethan — runs 5+ concurrent multi-day sessions across different projects.

**Approach:** Three decoupled components: (1) background processor that tails JSONL transcripts and extracts memories, (2) PreToolUse hook that injects relevant "memory bites" on each user message, (3) MCP tools for agent-initiated recall and expansion.

---

## Architecture Overview

```
JSONL Transcripts                    Shared SQLite DB (WAL)
(5+ active sessions)                 (episodes + chunks tables)
        │                                    ▲ write    │ read
        │ tail (byte offset)                 │          │
        ▼                                    │          ▼
┌─────────────────────┐             ┌─────────────────────────┐
│  ENGRAM PROCESSOR   │─── warm ───▶│  episodes table         │
│  (LaunchAgent)      │  extract    │  + existing chunks/fts  │
│                     │             └─────────────────────────┘
│  Tails all active   │                        │ read
│  JSONL files        │                        ▼
│                     │             ┌─────────────────────────┐
│  On new user msg:   │             │  PRETOOLUSE HOOK        │
│  search memory →    │             │  (per tool call)        │
│  write recollection │──────────▶  │                         │
│  file per session   │  writes     │  Reads recollection     │
│                     │  .json      │  file → injects bites   │
└─────────────────────┘             │  as additionalContext   │
                                    └─────────────────────────┘
                                               │
                                    ┌─────────────────────────┐
                                    │  MCP TOOLS              │
                                    │  memory_recall(query)   │
                                    │  memory_expand(id)      │
                                    │  (agent-initiated)      │
                                    └─────────────────────────┘
```

---

## Architecture Decisions

### Why a Separate Background Processor (not integrated into MCP server)

1. Must run even when no Claude session is active (to finish processing transcripts after a session pauses)
2. Must be a SINGLE instance watching ALL sessions (not one per session)
3. LaunchAgent with proper safeguards (PID file, SIGTERM handler, memory limits) is the proven macOS daemon pattern
4. Decoupled from MCP server lifecycle — MCP server starts/stops with Claude Code, processor runs independently

### Why PreToolUse for Recollection (not SessionStart or a new hook)

1. PreToolUse fires before every tool call — guaranteed to run during active conversation
2. Can inject `additionalContext` (proven pattern from flush nudge)
3. State file prevents double-injection per user message (inject once, skip subsequent tool calls in same turn)
4. SessionStart fires once per session start — useless for multi-day sessions where topics change

### Why Pre-Computed Recollections (not real-time search in hook)

1. PreToolUse hook must be FAST (<10ms) — loading an embedding model takes seconds
2. Background processor already has the model loaded (persistent process)
3. Processor pre-computes recollections on each new user message → writes to file
4. Hook just reads a small JSON file — zero search latency in the critical path

### Why Local Embeddings for Search (nomic-embed-text-v1.5)

1. Already set up and working in the MCP server
2. 768-dim, GGUF Q4 quantized — fast inference on CPU
3. No API dependency for the critical recollection path
4. API (GPT-4.1-nano) only used for warm-path extraction (non-critical, async)

---

## Schema Changes

### New `episodes` Table

```sql
CREATE TABLE IF NOT EXISTS episodes (
    id            TEXT PRIMARY KEY,           -- UUID
    session_id    TEXT NOT NULL,              -- source session UUID
    project       TEXT,                       -- project name (NULL for global)
    scope         TEXT NOT NULL DEFAULT 'project',  -- 'global' or 'project'
    summary       TEXT NOT NULL,              -- 1-2 sentence summary (the "memory bite")
    entities      TEXT,                       -- JSON array: ["file.ts", "WebSocket", "mutex"]
    importance    TEXT NOT NULL DEFAULT 'normal',    -- 'high' or 'normal'
    source_type   TEXT NOT NULL DEFAULT 'auto',      -- 'auto' (processor) or 'manual' (agent)
    full_content  TEXT,                       -- full context for expansion (2-3 paragraphs)
    embedding     BLOB,                       -- 768-dim float32 (nomic-embed-text-v1.5)
    created_at    INTEGER NOT NULL,           -- unix ms timestamp
    accessed_at   INTEGER NOT NULL,           -- unix ms timestamp (updated on retrieval)
    access_count  INTEGER NOT NULL DEFAULT 0  -- incremented on retrieval
);

CREATE INDEX IF NOT EXISTS idx_episodes_scope ON episodes(scope, project);
CREATE INDEX IF NOT EXISTS idx_episodes_session ON episodes(session_id);
```

### Schema Migration in `schema.ts`

**Bump schema version to 2.** Add a new migration block (guarded by version check):
- `PRAGMA busy_timeout = 5000;` (WAL concurrent access safety — add to BOTH MCP server and processor connections)
- `episodes` table creation (idempotent with IF NOT EXISTS)
- FTS5 virtual table for episodes: `episodes_fts` (same pattern as existing `fts`)
- Triggers for episodes_fts sync
- Update schema version marker to 2

---

## Files to Create / Modify

| File | Action | Description |
|------|--------|-------------|
| `mcp/schema.ts` | MODIFY | Add episodes table, episodes_fts, busy_timeout pragma, bump version to 2 |
| `processor/index.ts` | CREATE | Entry point: discover sessions (filter out subagents), start tailers, memory monitoring |
| `processor/session-tailer.ts` | CREATE | Tail a single JSONL, buffer messages, extract content from string OR array blocks |
| `processor/extractor.ts` | CREATE | Micro-batch LLM extraction: summary + entities + importance |
| `processor/recollection-writer.ts` | CREATE | Search memory, topic-change gate, atomic write top-3 bites to session file |
| `processor/state.ts` | CREATE | Persist byte offsets, processed message UUIDs per session |
| `hooks/pretooluse-recollection.ts` | CREATE | Read recollection file, dedup by messageUuid, inject bites |
| `mcp/server.ts` | MODIFY | Add memory_recall and memory_expand tools |
| `scripts/com.ethangabis.engram.plist` | CREATE | LaunchAgent plist for processor daemon |
| `scripts/engram-start.sh` | CREATE | Wrapper script that sources .env before launching processor |
| `package.json` | MODIFY | Add @logdna/tail-file dependency, add processor script |
| `~/.claude/settings.json` | MODIFY | Add second PreToolUse hook entry for recollection |

---

## Component 1: Engram Processor (`processor/`)

### `processor/index.ts` — Entry Point

```
1. Write PID file to ~/.claude-memory/engram.pid (with stale detection)
2. Register SIGTERM/SIGINT handler for graceful shutdown
3. Load embedding model (nomic-embed-text-v1.5 via node-llama-cpp)
   NOTE: This is a SECOND instance of the model (~200MB) separate from MCP server.
   Accepted trade-off: simpler architecture at cost of ~400MB total RAM.
   Both processes should use the same GGUF file path to benefit from OS page cache sharing.
4. Open SQLite DB (WAL mode, busy_timeout=5000)
5. Discover active sessions:
   - Glob ~/.claude/projects/*/*.jsonl (top-level project sessions)
   - EXCLUDE paths containing /subagents/ (subagent transcripts are noise)
   - Filter to files modified within last 7 days (not 24h — sessions last days)
   - Start a SessionTailer for each
6. Watch ~/.claude/projects/ with chokidar for NEW .jsonl files
   - On new file: skip if path contains /subagents/, else start a SessionTailer
7. Run periodic tasks:
   - Every 60s: scan for new/stale sessions
   - Every 4 hours: cold-path consolidation (merge duplicates, update MEMORY.md)
8. On SIGTERM: save all byte offsets, close DB, remove PID file, exit
```

**Zombie Prevention:**
- PID file at `~/.claude-memory/engram.pid` — on startup, check if PID exists and is alive
- If stale PID: overwrite. If alive: exit (only one instance allowed)
- SIGTERM handler saves state before exit
- LaunchAgent plist sets `KeepAlive: true` (restart on crash)
- Application-level memory monitoring: track RSS via `process.memoryUsage()`, log warning at 400MB, graceful restart at 512MB (macOS LaunchAgent doesn't support MemoryLimit)

### `processor/session-tailer.ts` — JSONL File Tailer

```
Class SessionTailer:
  constructor(jsonlPath, stateStore, embedModel, db)

  start():
    1. Read saved byte offset from stateStore (or 0 if new)
    2. Open file with @logdna/tail-file at saved offset
    3. Parse each new line as JSON
    4. Filter: only process type="user" and type="assistant"
    5. CONTENT EXTRACTION (C2 fix — content can be string OR array of blocks):
       - If entry.message.content is a string: use as-is
       - If entry.message.content is an array: extract text blocks only
         → filter blocks where block.type === "text"
         → join block.text values with "\n"
       - Skip entries where extracted text is empty
    6. Buffer messages in memory (ring buffer, max 50 messages)
    7. On new user message:
       a. Trigger recollection writer (search memory → write bites file)
       b. Check warm-path triggers:
          - messageCount >= 15 since last extraction? → extract
          - timeSinceLastExtraction >= 20 minutes? → extract
       c. Increment message counter

  extract():
    1. Take buffered messages since last extraction
    2. Pass to Extractor with {previousSummary, newMessages}
    3. Extractor returns candidate memories
    4. For each candidate: ADD/UPDATE/NOOP against episodes table
    5. Reset message counter and timer
    6. Save byte offset to stateStore

  stop():
    1. Save byte offset
    2. Close tail stream
```

**Session ID extraction:** Parse from the JSONL file path — format is `~/.claude/projects/<project-hash>/<session-uuid>.jsonl`. The filename (without extension) IS the session UUID.

**Session detection:** A session's JSONL is "active" if modified within the last 5 minutes. Tailers for stale sessions are paused (not destroyed) and resumed if the file gets new writes.

### `processor/extractor.ts` — Memory Extraction

```
async function extractMemories(
  previousSummary: string,
  newMessages: Array<{role: string, content: string}>,
  projectName: string | null,
  llmApiKey: string
): Promise<CandidateMemory[]>

Uses GPT-4.1-nano with prompt:
  "You are a memory extraction system. Given the previous context summary
   and new conversation messages, extract the key memories worth remembering.

   For each memory, provide:
   - summary: 1 sentence (max 40 tokens) — the 'memory bite'
   - full_content: 2-3 paragraphs of context for expansion
   - entities: array of key names (files, projects, concepts, people)
   - importance: 'high' if it's an architectural decision, user preference,
     convention, or critical bug fix. 'normal' for everything else.
   - scope: 'global' if it's a general fact/preference, 'project' if specific

   Also return an updated rolling summary of the conversation so far."

Returns: { memories: CandidateMemory[], updatedSummary: string }
```

**ADD/UPDATE/NOOP Logic:**
For each candidate memory:
1. Search existing episodes by embedding similarity (cosine > 0.85)
2. If match found AND same topic: UPDATE (replace summary, merge full_content, bump access)
3. If match found BUT different aspect: ADD (new episode)
4. If no match: ADD (new episode)
5. Never DELETE automatically — only compress during cold-path consolidation

**Cost:** ~$0.007 per extraction call (50K input tokens × $0.10/M + 5K output × $0.40/M). At one extraction per 15 messages, typical session = 3-5 extractions/day = $0.02-0.04/day.

### `processor/recollection-writer.ts` — Pre-Compute Recollections

```
async function writeRecollections(
  sessionId: string,
  userMessage: string,
  userMessageUuid: string,
  projectName: string | null,
  previousEmbedding: Float32Array | null,
  embedModel: EmbedProvider,
  db: Database
): { embedding: Float32Array }

1. Embed userMessage using local model (~5ms)
2. TOPIC-CHANGE GATE (I2 fix):
   - If previousEmbedding exists: compute cosine similarity
   - If cosine sim > 0.85 (same topic): SKIP writing new recollections
     (keep the previous recollection file as-is — avoids redundant memory bites)
   - If cosine sim <= 0.85 OR no previousEmbedding: proceed to step 3
   - Return the current embedding so caller can pass it to next invocation
3. Search episodes table:
   - BM25 + vector hybrid (same as existing search.ts logic)
   - Filter: scope='global' OR project=projectName
   - Sort by: 0.5*relevance + 0.3*recency + 0.2*accessFrequency
   - Limit: top 3
4. Format each result as a memory bite:
   {
     id: episode.id,
     bite: "[Memory flash: {episode.summary}]",
     date: episode.created_at,
     importance: episode.importance
   }
5. ATOMIC WRITE (C5 fix):
   - Write to temp file: ~/.claude-memory/recollections/<sessionId>.tmp
   - Then fs.renameSync() to ~/.claude-memory/recollections/<sessionId>.json
   - rename is atomic on macOS — hook never reads a half-written file
   Content:
   {
     messageUuid: userMessageUuid,
     timestamp: Date.now(),
     bites: [...]
   }
```

**High-importance memories get a floor:** Episodes with `importance='high'` get a minimum relevance score of 0.3, ensuring architectural decisions and user preferences always surface when even slightly relevant.

### `processor/state.ts` — Persistent State

```
State file: ~/.claude-memory/engram-state.json
{
  sessions: {
    "<sessionId>": {
      byteOffset: 12345,
      lastExtractedAt: 1740000000000,
      messagesSinceExtraction: 7,
      rollingSummary: "Working on WebSocket handler...",
      lastUserMessageUuid: "abc-123"
    }
  }
}

Saved: on every extraction + on SIGTERM + every 60 seconds
Loaded: on startup (enables crash recovery)
```

---

## Component 2: PreToolUse Recollection Hook (`hooks/pretooluse-recollection.ts`)

```
#!/usr/bin/env bun

1. Read stdin (hook input JSON)
2. SESSION ID RESOLUTION (C1 fix — three fallback strategies):
   a. Parse stdin JSON → extract session_id from the hook payload
      (PreToolUse stdin includes session context)
   b. Fallback: CLAUDE_SESSION_ID environment variable
   c. Fallback: scan ~/.claude-memory/recollections/ for most recently modified .json file
   If no session_id found by any method: exit silently (no output)
3. Read ~/.claude-memory/recollections/<session_id>.json
   - If missing or empty: exit silently (no output)
4. DEDUP BY MESSAGE UUID (I3 fix — replaces fragile boolean flag):
   - Read state file: ~/.claude-memory/recollections/<session_id>.state
   - State contains: { lastInjectedMessageUuid: "..." }
   - If recollection.messageUuid === lastInjectedMessageUuid: exit silently
     (already injected for this user message — skip on subsequent tool calls)
   - If different: proceed to inject and update state file
5. Write updated state: { lastInjectedMessageUuid: recollection.messageUuid }
6. Format bites into additionalContext string:

   "You have memories related to this conversation:
    {bite1}
    {bite2}
    {bite3}
    If any of these are relevant, you can call memory_expand(id) to
    recall the full context. Otherwise, continue your work."

7. Output JSON:
   {
     hookSpecificOutput: {
       hookEventName: "PreToolUse",
       additionalContext: <formatted string>,
       permissionDecision: "allow"
     }
   }
```

**Performance:** Only file reads + writes. No embedding, no DB queries, no API calls. Target: <5ms execution time.

**Settings.json registration:**
```json
{
  "type": "command",
  "command": "test -d ~/.claude-memory/recollections && bun /Users/ethangabis/Desktop/Projects/claude-memory/hooks/pretooluse-recollection.ts || true"
}
```

---

## Component 3: MCP Server Enhancement (`mcp/server.ts`)

### New Tool: `memory_recall`

```
{
  name: "memory_recall",
  description: "Get brief memory recollections for a topic. Returns short 'memory bites' that you can expand with memory_expand(). Use this when you want to check what you remember about a topic.",
  inputSchema: {
    query: string (required),
    limit: number (optional, default 5),
    project: string (optional)
  }
}

Handler:
1. Embed query using provider chain
2. Search episodes table (BM25 + vector hybrid)
3. Update accessed_at and access_count for returned episodes
4. Return formatted list:
   [1] (Feb 20, high) WebSocket race condition fixed with mutex on connection map — ID: ep_abc123
   [2] (Feb 18, normal) MM project uses PostgreSQL, not SQLite — ID: ep_def456
   ...
```

### New Tool: `memory_expand`

```
{
  name: "memory_expand",
  description: "Expand a memory recollection to get full context. Pass the episode ID from memory_recall results.",
  inputSchema: {
    id: string (required) — episode ID
  }
}

Handler:
1. SELECT * FROM episodes WHERE id = ?
2. Update accessed_at and access_count
3. Return full content:
   ## Memory: WebSocket Race Condition Fix
   **Date:** Feb 20, 2026
   **Project:** TrueTTS
   **Importance:** high

   **Summary:** Fixed a race condition in the WebSocket handler by adding a mutex on the connection map in server.ts:142.

   **Full Context:**
   {episode.full_content}

   **Related Entities:** server.ts, WebSocket, mutex, connection map
```

---

## Component 4: LaunchAgent (`scripts/com.ethangabis.engram.plist`)

**Wrapper script approach (C4 fix):** LaunchAgent calls a wrapper script that sources environment variables (including OPENAI_API_KEY) from `~/.claude-memory/.env` before launching the processor. No hardcoded API keys in plist.

### `scripts/engram-start.sh` — Wrapper Script
```bash
#!/bin/bash
# Source environment (API keys, etc.)
[ -f "$HOME/.claude-memory/.env" ] && source "$HOME/.claude-memory/.env"
exec /Users/ethangabis/.bun/bin/bun /Users/ethangabis/Desktop/Projects/claude-memory/processor/index.ts
```

### `scripts/com.ethangabis.engram.plist` — LaunchAgent Plist
```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
    <key>Label</key>
    <string>com.ethangabis.engram</string>
    <key>ProgramArguments</key>
    <array>
        <string>/bin/bash</string>
        <string>/Users/ethangabis/Desktop/Projects/claude-memory/scripts/engram-start.sh</string>
    </array>
    <key>RunAtLoad</key>
    <true/>
    <key>KeepAlive</key>
    <true/>
    <key>StandardOutPath</key>
    <string>/Users/ethangabis/.claude-memory/engram.stdout.log</string>
    <key>StandardErrorPath</key>
    <string>/Users/ethangabis/.claude-memory/engram.stderr.log</string>
    <key>SoftResourceLimits</key>
    <dict>
        <key>NumberOfFiles</key>
        <integer>256</integer>
    </dict>
    <key>ProcessType</key>
    <string>Background</string>
    <key>ThrottleInterval</key>
    <integer>5</integer>
</dict>
</plist>
```

**Note:** macOS LaunchAgent does not support MemoryLimit on modern macOS (I6 fix). Memory monitoring is done at the application level inside `processor/index.ts` (see Zombie Prevention section above).

---

## Implementation Tasks & Dependencies

```
Task 1: Schema migration (episodes table + busy_timeout)
    │
    ├──▶ Task 2: Engram Processor (processor/ — all 5 files)
    │
    ├──▶ Task 3: PreToolUse Recollection Hook
    │
    └──▶ Task 4: MCP Server (memory_recall + memory_expand)

Task 5: Settings.json + LaunchAgent registration (after Tasks 2-4)
```

- Tasks 2, 3, 4 are **independent** (run in parallel) but all depend on Task 1
- Task 5 depends on Tasks 2 and 3

### Task Breakdown

| # | Task | Files | Agent Type | Depends On |
|---|------|-------|-----------|-----------|
| 1 | Schema migration (version 2) | `mcp/schema.ts` | general-purpose | — |
| 2 | Engram Processor | `processor/*.ts` (5 files), `package.json` | general-purpose | 1 |
| 3 | PreToolUse Recollection Hook | `hooks/pretooluse-recollection.ts` | general-purpose | 1 |
| 4 | MCP Server Enhancement | `mcp/server.ts` | general-purpose | 1 |
| 5 | Registration & Config | `settings.json`, `plist`, `engram-start.sh`, `~/.claude-memory/.env` | general-purpose | 2, 3 |

---

## Testing Strategy

### Manual Tests

1. **Processor starts and tails active session:**
   - Start processor → verify it finds current session's JSONL
   - Send a message in Claude → verify processor logs new message detection
   - Verify recollection file created at `~/.claude-memory/recollections/<sessionId>.json`

2. **Recollection injection works:**
   - Verify Claude receives memory bites in additionalContext
   - Verify bites are only injected ONCE per user message (not repeated on subsequent tool calls)
   - Verify no injection when recollection file is missing

3. **Memory extraction works:**
   - Have a conversation about a specific topic
   - Wait for warm-path trigger (15 messages or 20 minutes)
   - Check episodes table for extracted memories
   - Verify summary is <40 tokens, entities are extracted, scope is correct

4. **Multi-session sharing:**
   - Open two sessions on different projects
   - Save a global memory in Session A
   - Verify Session B's recollections include the new global memory

5. **MCP tools work:**
   - Call `memory_recall("WebSocket")` → verify brief results
   - Call `memory_expand("ep_abc123")` → verify full content returned

6. **Crash recovery:**
   - Kill processor (SIGKILL)
   - Restart → verify it resumes from saved byte offsets (no duplicate processing)

7. **Zombie prevention:**
   - Start processor twice → verify second instance exits immediately
   - Kill processor → verify PID file is cleaned up

### Regression Tests (existing system)

All existing hooks must continue working:
- SessionStart context injection
- Stop hook (daily log + nudge)
- PreCompact flush chain (StatusLine → flush-marker → PreToolUse nudge → PreCompact)
- Memory MCP tools (memory_search, memory_get, memory_save)

---

## Risk Assessment

| Risk | Severity | Mitigation |
|------|----------|------------|
| Processor zombies (claude-mem repeat) | HIGH | PID file + stale detection + SIGTERM handler + app-level memory monitoring |
| SQLite contention (5 sessions + processor) | MEDIUM | WAL mode + busy_timeout=5000 in BOTH MCP and processor + BEGIN IMMEDIATE |
| API downtime stalls extraction | LOW | Circuit breaker: skip extraction, retry next trigger. Recollection still works (searches existing memories) |
| Context rot from recollections | LOW | Max 3 bites per injection, ~120 tokens total, once per user message, topic-change gated |
| Embedding model memory usage | MEDIUM | ~400MB total (two instances: MCP + processor). Same GGUF path → OS page cache sharing. |
| Stale recollection files | LOW | Processor cleans files for sessions inactive >1 hour |
| Race condition on recollection file | LOW | Atomic write via temp file + fs.renameSync() |
| Subagent transcript noise | LOW | Paths containing /subagents/ excluded from glob + chokidar watch |

---

## Rollback Plan

Each component is independently disableable:
1. **Processor:** `launchctl unload com.ethangabis.engram.plist` — stops all background processing
2. **Recollection hook:** Remove the PreToolUse entry from settings.json — stops all injection
3. **MCP tools:** Tools are additive — removing them doesn't affect existing memory_search/memory_get/memory_save
4. **Schema:** Episodes table is additive — existing chunks/fts tables unchanged

---

## Code Review Findings (Applied)

Plan was reviewed with score 6/10. All CRITICAL issues have been addressed:

| ID | Issue | Fix Applied |
|----|-------|-------------|
| C1 | PreToolUse hook can't reliably get session_id | Three-fallback strategy: parse stdin JSON → CLAUDE_SESSION_ID env → scan recollections dir |
| C2 | JSONL content can be string OR array of blocks | Explicit content extraction: check type, filter text blocks, join |
| C3 | Glob matches subagent JSONL files (noise) | Exclude paths containing `/subagents/` from both glob discovery and chokidar watch |
| C4 | Dual embedding model (~400MB) + API key placeholder | Accepted RAM trade-off (documented), wrapper script sources .env for API key |
| C5 | Race condition on recollection file writes | Atomic write via temp file + fs.renameSync() |

IMPORTANT suggestions incorporated:

| ID | Suggestion | Fix Applied |
|----|-----------|-------------|
| I2 | No topic-change gating (contradicts research) | Added cosine similarity gate (>0.85 = skip) in recollection-writer |
| I3 | `injected` boolean flag is fragile | Replaced with messageUuid tracking in separate .state file |
| I4 | Schema version not bumped | Bumped to version 2 with migration guard |
| I5 | busy_timeout needed in both processes | Documented: add to BOTH MCP server and processor DB connections |
| I6 | macOS LaunchAgent doesn't support MemoryLimit | Application-level memory monitoring via process.memoryUsage() |
| I7 | 24h filter too aggressive for multi-day sessions | Changed to 7-day filter |

MINOR notes (deferred to implementation):
- M1: Log rotation for stdout/stderr (use logrotate or periodic truncation)
- M2: Metrics/observability (add later if needed)
- M3: Graceful degradation docs for users
