# Engram V2 -- Human-Like Memory for Claude Code

**Date:** 2026-02-23
**Status:** Implemented
**Plan:** [docs/plans/2026-02-23-engram-v2.md](../plans/2026-02-23-engram-v2.md)
**R&D:** [architecture](../research/2026-02-23-human-like-memory-architecture.md) | [validation](../research/2026-02-23-engram-validation.md) | [constraints](../research/2026-02-23-continuous-memory-constraints.md)

## Overview
Automatic memory capture from all Claude Code sessions + continuous "memory bite" recollections injected during conversation. Three decoupled components: background processor, PreToolUse hook, and MCP tools.

## How It Works
1. Engram Processor (LaunchAgent daemon) tails all active JSONL transcript files
2. On each user message: searches existing memories, writes top 3 "memory bites" to a JSON file
3. PreToolUse hook reads the file and injects bites into Claude's context as additionalContext
4. Claude sees bites and can call memory_expand(id) for full context, or ignore them
5. Every 15 messages or 20 minutes: GPT-4.1-nano extracts new memories as episodes in SQLite
6. All sessions share the same database -- global memories + project-specific memories

## Files Changed
| File | Change Type | Description |
|------|-------------|-------------|
| mcp/schema.ts | modified | Episodes table with FTS5, triggers for FTS sync, busy_timeout=5000, schema version bumped to 2 |
| processor/index.ts | created | Daemon entry point -- PID file with stale detection, session discovery (excludes subagents), chokidar file watcher, memory monitoring (warn at 400MB, restart at 512MB) |
| processor/session-tailer.ts | created | JSONL tail via fs.watch with 200ms debounce, byte-offset resume, content extraction from string or array blocks, ring buffer (max 50 messages), warm-path trigger checks |
| processor/extractor.ts | created | GPT-4.1-nano memory extraction with structured JSON output, ADD/UPDATE/NOOP dedup via cosine similarity (>0.85 threshold), episode snapshot pre-fetch to avoid N full-table scans |
| processor/recollection-writer.ts | created | Topic-change gated recollection pre-computation (cosine sim >0.85 = skip), hybrid BM25+vector scoring (0.5 relevance + 0.3 recency + 0.2 access frequency), high-importance floor at 0.3 |
| processor/state.ts | created | Persistent byte offsets and crash recovery via engram-state.json, periodic save every 60s, atomic writes via temp file + rename |
| hooks/pretooluse-recollection.ts | created | Recollection injection hook with three-fallback session ID resolution (stdin JSON, env var, most-recent file scan), messageUuid dedup via .state file |
| mcp/server.ts | modified | Added memory_recall tool (hybrid BM25+vector search over episodes, returns formatted bite list) and memory_expand tool (full episode context with entities, project, importance) |
| scripts/engram-start.sh | created | LaunchAgent wrapper script that sources ~/.claude-memory/.env before launching processor with bun |
| scripts/com.ethangabis.engram.plist | created | LaunchAgent configuration: RunAtLoad, KeepAlive, Background ProcessType, 1024 file descriptor limit, ThrottleInterval=5 |
| ~/.claude/settings.json | modified | Added PreToolUse recollection hook entry with guard: test -d recollections dir before running |
| ~/.claude-memory/.env | created | API key configuration (OPENAI_API_KEY) |
| package.json | modified | Added openai and chokidar dependencies, added "processor" script |

## Architecture Decisions
- Background processor runs independently of Claude sessions (LaunchAgent) -- must run even when no session is active to finish processing transcripts after a session pauses
- Pre-computed recollections for <5ms hook performance -- processor writes JSON file, hook only reads it (no DB, no embedding, no API calls in critical path)
- Topic-change gate prevents redundant injections (cosine sim > 0.85 between consecutive user messages = skip writing new recollections)
- MessageUuid dedup via separate .state file ensures one injection per user message across multiple PreToolUse firings in the same turn
- Atomic file writes (temp file + fs.renameSync) prevent race conditions between processor writes and hook reads
- SQLite WAL mode with busy_timeout=5000 set in initDb for all connections (MCP server and processor share the same DB safely)
- Binary importance (high/normal) instead of unreliable 1-10 LLM scoring -- validated by research showing numeric LLM scores lack consistency
- No association graph (Mem0 showed only ~2% accuracy gain over base approach -- worst effort-to-value ratio per validation research)
- App-level memory monitoring via process.memoryUsage() instead of unsupported macOS LaunchAgent MemoryLimit -- warn at 400MB, graceful restart at 512MB
- Session discovery excludes paths containing /subagents/ to avoid processing noise from subagent transcripts
- 7-day file age filter for session discovery (not 24h) because sessions regularly last multiple days
- Episode snapshot pre-fetch: fetchEpisodeSnapshot() called once per extraction batch, then passed to each upsertEpisode call to avoid N full-table scans
- fs.watch with 200ms debounce (not @logdna/tail-file) for JSONL tailing -- simpler, works well with Bun, avoids extra dependency
- Singleton OpenAI client in extractor to avoid recreating HTTP connection pool on every extraction call
- Extraction backoff on API failure: sets lastExtractedAt 5 minutes in the future to prevent every subsequent user message from retrying

## Known Limitations
- FTS5 indexes episode summaries only, not entity names (vector search still covers entities via embedding similarity)
- Cold-path consolidation (episodic-to-semantic graduation) is a placeholder -- logged every 4 hours but not yet implemented
- Project names are filesystem path hashes (from ~/.claude/projects/<hash>/), not human-readable
- Log rotation not implemented for daemon stdout/stderr (engram.stdout.log, engram.stderr.log)
- User must manually add OPENAI_API_KEY to ~/.claude-memory/.env for extraction to work (recollection still works without it using existing episodes)
- No graceful degradation if embedding model fails to load -- processor continues but recollections will be limited to BM25-only
- Recollection file cleanup for stale sessions not yet implemented (files persist in ~/.claude-memory/recollections/ indefinitely)

## Setup Instructions
1. Add your OpenAI API key: edit ~/.claude-memory/.env, set the OPENAI_API_KEY value
2. Install the LaunchAgent: cp scripts/com.ethangabis.engram.plist ~/Library/LaunchAgents/ && launchctl load ~/Library/LaunchAgents/com.ethangabis.engram.plist
3. Verify processor is running: launchctl list | grep engram
4. The PreToolUse hook is already registered in settings.json
5. The MCP tools (memory_recall, memory_expand) are available immediately

## Testing Notes
- Start a conversation, verify recollection file appears at ~/.claude-memory/recollections/<sessionId>.json
- Check processor logs: tail -f ~/.claude-memory/engram.stderr.log
- After 15+ messages, verify episodes appear: sqlite3 ~/.claude-memory/memory.db "SELECT id, summary, importance FROM episodes LIMIT 10"
- Test memory_recall("topic") and memory_expand("ep_xxx") in Claude
- Kill processor and restart -- verify crash recovery from saved byte offsets in ~/.claude-memory/engram-state.json
- Start processor twice -- verify second instance exits immediately (PID file check)
- Verify subagent JSONL files are excluded from session discovery
- Verify topic-change gate: send similar messages in sequence, confirm recollection file is not rewritten when cosine sim > 0.85
